<h1>Machine Learning ðŸ”¢ ðŸ§ </h1>
<ol>
  <h2>General Concepts</h2>
  <li>
    <strong>What is the difference between supervised and unsupervised learning? Can you provide examples of each?</strong>
    <pre>
    Supervised: Regression, Classification.
    Unsupervised: Clustering.
    </pre>
  </li>
  <li>
    <strong>Can you explain the bias-variance tradeoff in machine learning? How do you address overfitting and underfitting?</strong>
    <pre>
    High-bias: an underfitting, too simple model.
    Bias: error â†’ linear regression model â†’ when trying to fit nonlinear relationship
    High-variance: overfitting â†’ complex model.
    Regularization â†’ adds a penalty term to the loss function of the model to discourage overfitting.
    Ensemble models: like bagging and boosting, which combine multiple models to reduce the variance of the prediction.
    </pre>
  </li>
  <li>
    <strong>What is regularization in machine learning? How does it help prevent overfitting?</strong>
    <pre>
    Adds penalty term to the modelâ€™s function,
    L1(lasso): add a penalty term to the sum of absolute values of the modelâ€™s parameters â†’ zero â†’ remove outliers
    L2(Ridge): add a penalty term to the sum of squared values of the model parameters
    </pre>
  </li>
  <li>
    <strong>What is the difference between classification and regression problems in machine learning?</strong>
    <pre>
    Classification: Discrete categorical label.
    Regression: Continuous numerical values.
    </pre>
  </li>
  <li>
    <strong>Can you explain the concept of deep learning? How is it different from traditional machine learning?</strong>
    <pre><b>Extract features.</b></pre>
  </li>
  <li>
    <strong>Is SVM linear or nonlinear, and how to convert it to nonlinear model?</strong>
    <pre>
    Kernal trick.
    </pre>
  </li>

  <h3>Gradient Descent</h3>
  <li>What is gradient descent, and how does it work?</li>
  <li>What are the types of gradient descent algorithms?</li>
  <li>What is the difference between batch gradient descent, stochastic gradient descent, and mini-batch gradient descent?</li>
  <li>What is the role of learning rate in gradient descent, and how do you choose the appropriate learning rate?</li>
  <li>What is the impact of the learning rate on the convergence of the gradient descent algorithm?</li>
  <li>How do you handle the problem of overshooting and undershooting in gradient descent?</li>
  <li>What is the role of momentum in gradient descent, and how does it help in convergence?</li>
  <li>What is the difference between gradient descent and Newton's method?</li>
  <li>What is the role of regularization in gradient descent?</li>
  <li>How do you handle the problem of local minima in gradient descent?</li>
  <li>What is the difference between convex and non-convex optimization problems in gradient descent?</li>
  <li>What is the impact of the batch size on the convergence of the gradient descent algorithm?</li>
  <li>How do you choose the appropriate batch size for the gradient descent algorithm?</li>
  <li>How do you handle the problem of vanishing gradients in deep learning?</li>
  <li>What is the impact of the initialization of the weights on the convergence of the gradient descent algorithm?</li>
  <li>
    <strong>Can you explain the difference between precision and recall in classification problems? How are they related to the concept of a confusion matrix?</strong>
  </li>
  <li>
    <strong>Can you describe the process of hyperparameter tuning? How do you decide which hyperparameters to adjust and by how much?</strong>
  </li>
  <li>
    <strong>Can you discuss some common evaluation metrics used in machine learning? How do you choose which metric to use for a particular problem?</strong>
  </li>
  <li>
    <strong>Can you explain the concept of ensemble learning in machine learning? How does it help improve the accuracy of a model?</strong>
  </li>
  <li>
    <strong>Can you discuss some common techniques for reducing the dimensionality of data in machine learning? Why are they important?</strong>
  </li>
  <li>
    <strong>Can you explain the difference between a linear and a nonlinear model in machine learning? When would you use one over the other?</strong>
  </li>
  <li>
    <strong>Can you describe the process of clustering in unsupervised learning? Can you provide an example of how it is used in practice?</strong>
  </li>
  <li>
    <strong>Can you explain the difference between L1 and L2 regularization in machine learning? How do they affect the model?</strong>
  </li>
  <li>
    <strong>Can you discuss some common techniques for handling missing data in machine learning? How do they work?</strong>
  </li>
  <li>
    <strong>Can you explain the concept of autoencoders in deep learning? How do they work?</strong>
  </li>
  <li>
    <strong>Can you describe the process of cross-validation in machine learning? Why is it important?</strong>
  </li>
  <li>
    <strong>Can you explain the difference between a local and a global minimum in optimization problems? How do you address the issue of getting stuck in a local minimum?</strong>
  </li>
  
</ol>
